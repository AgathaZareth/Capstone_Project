{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917341ed",
   "metadata": {},
   "source": [
    "# <center>Drafting New Talent for SF Giants 2023 Season\n",
    "## <center>Linear Regression of MLB teams' percentage of wins from the last 5 regular seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72da3c54",
   "metadata": {},
   "source": [
    "![close-up of a worn baseball on a lush green field](https://www.alumni.creighton.edu/s/1250/images/editor/alumni_site/emails/baseball_in_grass_cover_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74633ba1",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Intuitively, it makes sense that the performance of the team as a whole is more important than individual players themselves. We are all familiar with the idiom \"greater than the sum of its parts\" and baseball teams are no exception. This notebook will provide an understanding of how a teams cumulative statistics influence the percentage of wins in their regular season games. With this inferentail understanding there is also predictive capabilities, that is to say, the be able to take in the statistics of a team, then to *predict* that teams win percentage for their regular season. The effectiveness of this predictive model will be measured by how well it predicts win percentages in a test set; a set that I have the answers for but the model does not. \n",
    "\n",
    "The insight provided by the inferential aspects will guide my recruitment recommendations and the predictive ability will test the new rosters potential win percentage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d783915",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "<p><img src=https://i.pinimg.com/736x/0e/68/ed/0e68eda6243faa5f754b1cfb2b04846d--giants-sf-giants-baseball.jpg width=\"125\", alt=\"SF Giants logo\" style=\"float: left;vertical-align:middle;margin:0px 15px\">San Francisco Giants had an unremarkable 2022 season. This year SF Giants General Manager (Pete Putila), SF Giants Senior Director of Player Development (Kyle Haines), and Senior Director of Amatuer Scouting (Micheal Holmes) are looking to invest a huge portion of their efforts into recruiting from college and minor league levels. Beyond looking at an individual player's potential, they want predictions on the collective cohesiveness of a team and how the team as a whole will perform throughout the season. The most obvious metric to evaluate this is a teams percentage of wins during a regular season. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa0e8bb",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "---\n",
    "\n",
    "# <center>Overview of notebooks contained in this repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e066a9",
   "metadata": {},
   "source": [
    "## Sourcing Data\n",
    "I have sourced all my own data and did not use any premade datasets. \n",
    "All the data collected came from web scraping of various websites. Each set needed quite a bit of code to acquire, and then clean, so this resulted in several notebooks. To better understand my process I have a brief overview of what each notebook contains, below.\n",
    "\n",
    "### Web Scraping Player Stats\n",
    " - **Division I Collegiate Player Stats**  can be found in the [College_table notebook](https://github.com/AgathaZareth/Capstone_Project/blob/main/notebooks/College_table.ipynb). Here I got a list of Division I colleges from [TheBaseballCube.com](https://thebaseballcube.com). Then using this list I was able to select only division 1 college slugs from [D1Baseball.com](https://d1baseball.com). From there I was able to scrap players hitting stats for 2022 from each division 1 college. \n",
    "\n",
    " - **Triple-A Minor League Player Stats** can be found in the [MiLB_table notebook](https://github.com/AgathaZareth/Capstone_Project/blob/main/notebooks/MiLB_table.ipynb). From [MiLB.com](https://www.milb.com) I first got a list of team id numbers for the triple A teams, then I used those to change url slugs to get player hitting stats. Fortunately, this website defaults to showing qualified players only so the resulting data frame is much smaller because it is already filtered to just the relevant players. \n",
    "\n",
    " - **MLB Player Stats** can be found in the [MLB_5_seasons notebook](https://github.com/AgathaZareth/Capstone_Project/blob/main/notebooks/MLB_5_seasons.ipynb). From [MLB.com](https://www.mlb.com)  I swapped out years and page numbers in urls to get players hitting stats for 5 seasons. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f29bd4",
   "metadata": {},
   "source": [
    "#### <center> The above 3 mentioned player stats DF's contain the following data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296a5058",
   "metadata": {},
   "source": [
    "| Column     | Description   |\n",
    "|------------|:--------------|\n",
    "| `Team`                  | **Team abbreviation**  |\n",
    "| `Games Played`          | **Games in which a player has appeared.**  |\n",
    "| `At Bats`               | **Trips to the plate that do not result in a walk, hit by pitch, sacrifice, or reach on interference.**  |\n",
    "| `Runs`                  | **When a baserunner safely reaches home plate and scores.**  |\n",
    "| `Hits`                  | **When a batter reaches base safely on a fair ball unless the batter is deemed by the official scorer to have reached on an error or a fielder's choice.**  |\n",
    "| `Doubles`               | **When a batter reaches on a hit and stops at second base or only advances farther than second base on an error or a fielder's attempt to put out another baserunner.**  |\n",
    "| `Triples`               | **When a batter reaches on a hit and stops at third base or only advances farther than third base on an error or a fielder's attempt to put out another baserunner.**  |\n",
    "| `Home Runs`             | **When a batter reaches on a hit, touches all bases, and scores a run without a putout recorded or the benefit of error.**  |\n",
    "| `Runs Batted In`        | **Runs which score because of the batter's safe hit, sac bunt, sac fly, infield out or fielder's choice or is forced to score by a bases loaded walk, hit batter, or interference.**  |\n",
    "| `Walks`                 | **When a batter is awarded first base after four balls have been called by the umpire or the opposing team opts to intentionally award the batter first base.**  |\n",
    "| `Strikeouts`            | **When the umpire calls three strikes on the batter.**  |\n",
    "| `Stolen Bases`          | **When the runner advances one base unaided by a hit, a putout, an error, a force-out, a fielder's choice, a passed ball, a wild pitch, or a walk.**  |\n",
    "| `Caught Stealing`       | **When a runner attempts to steal but is tagged out before safely attaining the next base.**  |\n",
    "| `Batting Average`       | **The rate of hits per at bat against a pitcher. (formula: Hits/At Bats)**  |\n",
    "| `On-Base Percentage`    | **The rate at which a batter reached base in his plate appearances. (formula: (H+BB+HBP)/(AB+BB+HBP+SF) )**  |\n",
    "| `Slugging Percentage`   | **The rate of total bases per at bat. (formula: (1B+2Bx2+3Bx3+HRx4)/At Bats)**  |\n",
    "| `On-Base Plus Slugging` | **The sum of on-base percentage and slugging percentage. (formula: On-Base Percentage+Slugging Percentage)**  |\n",
    "| `Year`                  | **Year**  |\n",
    "| `Player Name`           | **Player's name**  |\n",
    "| `Position`              | **Position of player**  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a980d",
   "metadata": {},
   "source": [
    "### Web Scraping Game Stats\n",
    " - **MLB Game Stats** can be found in [Games_by_day notebook](https://github.com/AgathaZareth/Capstone_Project/blob/main/notebooks/Games_by_day.ipynb). Also from [MLB.com](https://www.mlb.com), I collected data on each game of the regular seasons. I was able to create dataframes for each season.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92fe11e",
   "metadata": {},
   "source": [
    "| Column     | Description   |\n",
    "|------------|:--------------|\n",
    "| `Day`                  | **Day of the week**  |\n",
    "| `Month`                | **Month Abbreviation**  |\n",
    "| `Date`                 | **Date of the month**  |\n",
    "| `Away`                 | **Away team**  |\n",
    "| `Home`                 | **Home team**  |\n",
    "| `Win`                  | **Winning team**  |\n",
    "| `W Score`              | **Winning teams score**  |\n",
    "| `Lose`                 | **Losing team**  |\n",
    "| `L Score`              | **Losing teams score**  |\n",
    "| _`Year`_               | _**each df was saved by year/season and `year` was added later and combined with `Month` and `Date` and converted to YYYY-MM-DD format**_  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc4494a",
   "metadata": {},
   "source": [
    "### Creating Team Stats\n",
    " - **MLB Team Stats** can be found in the [Aggregate_team_stats](https://github.com/AgathaZareth/Capstone_Project/blob/main/notebooks/Aggregate_team_stats.ipynb) notebook. This is where I combined the MLB player and game stats into one df. First, I used the MLB player stats and filtered out players with less than 5 at bats. Next I found the cumulative totals of the players on a team, as well as their averages. Secondly, I used MLB game stats to get teams number of wins and losses for each season, to get the win percentage, per team, per season. Finally, I added the win percentages to the aggregated stats table. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba294f55",
   "metadata": {},
   "source": [
    "| Column     | Description   |\n",
    "|------------|:--------------|\n",
    "| `Team`                      | **Team abbreviation**  |\n",
    "| `Year`                      | **Year/Season**  |\n",
    "| `Games Played Sum`          | **Cumulative sum of games played.**  |\n",
    "| `At Bats Sum`               | **Cumulative sum of trips to the plate that do not result in a walk, hit by pitch, sacrifice, or reach on interference.**  |\n",
    "| `Runs Sum`                  | **Cumulative sum of when a baserunner safely reaches home plate and scores.**  |\n",
    "| `Hits Sum`                  | **Cumulative sum of when a batter reaches base safely on a fair ball unless the batter is deemed by the official scorer to have reached on an error or a fielder's choice.**  |\n",
    "| `Doubles Sum`               | **Cumulative sum of when a batter reaches on a hit and stops at second base or only advances farther than second base on an error or a fielder's attempt to put out another baserunner.**  |\n",
    "| `Triples Sum`               | **Cumulative sum of when a batter reaches on a hit and stops at third base or only advances farther than third base on an error or a fielder's attempt to put out another baserunner.**  |\n",
    "| `Home Runs Sum`             | **Cumulative sum of when a batter reaches on a hit, touches all bases, and scores a run without a putout recorded or the benefit of error.**  |\n",
    "| `Runs Batted In Sum`        | **Cumulative sum of runs which score because of the batter's safe hit, sac bunt, sac fly, infield out or fielder's choice or is forced to score by a bases loaded walk, hit batter, or interference.**  |\n",
    "| `Walks Sum`                 | **When a batter is awarded first base after four balls have been called by the umpire or the opposing team opts to intentionally award the batter first base.**  |\n",
    "| `Strikeouts Sum`            | **Cumulative sum of when the umpire calls three strikes on the batter.**  |\n",
    "| `Stolen Bases Sum`          | **Cumulative sum of when the runner advances one base unaided by a hit, a putout, an error, a force-out, a fielder's choice, a passed ball, a wild pitch, or a walk.**  |\n",
    "| `Caught Stealing Sum`       | **Cumulative sum of when a runner attempts to steal but is tagged out before safely attaining the next base.**  |\n",
    "| `Mean Games Played`         | **Average number of Games played.**  |\n",
    "| `Mean At Bats`              | **Average number of trips to the plate that do not result in a walk, hit by pitch, sacrifice, or reach on interference**  |\n",
    "| `Mean Runs`                 | **Average number of runs when a baserunner safely reaches home plate and scores .**  |\n",
    "| `Mean Hits`                 | **Average number of times when a batter reaches base safely on a fair ball unless the batter is deemed by the official scorer to have reached on an error or a fielder's choice.**  |\n",
    "| `Mean Doubles`              | **Average number of times when a batter reaches on a hit and stops at second base or only advances farther than second base on an error or a fielder's attempt to put out another baserunner.**  |\n",
    "| `Mean Triples`              | **Average number of times when a batter reaches on a hit and stops at third base or only advances farther than third base on an error or a fielder's attempt to put out another baserunner.**  |\n",
    "| `Mean Home Runs`            | **Average number of times a batter reaches on a hit, touches all bases, and scores a run without a putout recorded or the benefit of error.**  |\n",
    "| `Mean Runs Batted In`       | **Average number of runs which score because of the batter's safe hit, sac bunt, sac fly, infield out or fielder's choice or is forced to score by a bases loaded walk, hit batter, or interference.**  |\n",
    "| `Mean Walks`                | **Average number of times a batter is awarded first base after four balls have been called by the umpire or the opposing team opts to intentionally award the batter first base.**  |\n",
    "| `Mean Strikeouts`           | **Average number of times when the umpire calls three strikes on the batter.**  |\n",
    "| `Mean Stolen Bases`         | **Average number of times when the runner advances one base unaided by a hit, a putout, an error, a force-out, a fielder's choice, a passed ball, a wild pitch, or a walk.**  |\n",
    "| `Mean Caught Stealing`      | **Average number of times when a runner attempts to steal but is tagged out before safely attaining the next base.**  |\n",
    "| `% wins`      | **The percentage of wins of regular season games.**  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee8df80",
   "metadata": {},
   "source": [
    "## Making Predictions and Recommendations\n",
    "    \n",
    "Once I created the data I needed I was able to start a new modeling notebook. That is where we are now. This notebook contains exploratory visualizatios, modeling to predict a team win rates over regular seasons, gives interpretations of the model findings, offers recommendations for stake holders, and gvies next step suggestiongs for how to expand on this work. ????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b09b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "# <center> Modeling Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851ff57",
   "metadata": {},
   "source": [
    "## 1. Data Understanding\n",
    "The data comes from web scraping [MLB.com](https://www.mlb.com/stats/). I took the last 5 years of players hitting stats over regular seasons and cumulated them into team stats. I also took game details to determine team win percentages per season. To avoid collinearity issues down the line I only addeded staticsi that did not have any direct relationship to other stats, ie I did not include things that already combined other stats. For example, batting average combines `hits` and `at bats` so I included hits and at bats but left out batting average. Any player stats with a formula was left out. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967fc641",
   "metadata": {},
   "source": [
    "## 2. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d4b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "\n",
    "# statsmodels\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# scipy\n",
    "import scipy.stats as stats\n",
    "\n",
    "#visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "seed = 137"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5174b6",
   "metadata": {},
   "source": [
    "## 3. Functions\n",
    "I like to put any thing that is used more than once into a function to avoid the copy past look of the notebook. Additionally, this keeps the flow of the notebook smoother and just generally look cleaner. Each function has been given a descriptive name so it is fairly obvious what it is being used for. Docstring will elaborated when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69b726",
   "metadata": {},
   "source": [
    "### 3a. Visualization functions\n",
    "Many of my visualizations are used more than once or require a big chunck code so functions keep the notebook cleaner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50991c01",
   "metadata": {},
   "source": [
    "#### 3a - 1. `hist_grid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d77a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_grid(df, ncols, nrows, figsize, title=None, title_fontsize=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Uses \n",
    "    \n",
    "    Output\n",
    "    ----------\n",
    "    Grid of histogram tiles. Each tile is a feature from a specified dataframe. \n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # setup\n",
    "    sns.set(font_scale=.8)\n",
    "    #sns.set_style('darkgrid', \n",
    "    #              {'axes.facecolor': '0.9', \"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=figsize)\n",
    "    fig.set_tight_layout(True)\n",
    "    if title != None:\n",
    "        if title_fontsize != None:\n",
    "            fig.suptitle(title, fontsize=title_fontsize)\n",
    "        else:\n",
    "            fig.suptitle(title)\n",
    "\n",
    "    # plot\n",
    "    for index, col in enumerate(df.columns):\n",
    "        ax = axes[index//ncols][index%ncols]\n",
    "        sns.histplot(data=df[col], ax=ax, linewidth=0.1, alpha=1)\n",
    "        #ax.tick_params(axis='x', rotation=30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db529c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e4a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00978df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c08f5fe",
   "metadata": {},
   "source": [
    "## 4. The Raw Data\n",
    "In the cell below I load the dataframe created in the [Aggregate_team_stats](https://github.com/AgathaZareth/Capstone_Project/blob/main/notebooks/Aggregate_team_stats.ipynb) notebook. See above SECTION for list of column names and description. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021673a",
   "metadata": {},
   "source": [
    "### 4a. Load and visually check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a717d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole = pd.read_pickle(\"../pickled_tables/MODELING_DF.pkl\")\n",
    "whole.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e4211",
   "metadata": {},
   "source": [
    "### 4b. Drop unnecessary `Team` & `Year` columns\n",
    "I do not want the model looking for trends in the `Year` or `Team` features so these need to be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2cce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = whole.drop(['Team', 'Year'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba0e9e5",
   "metadata": {},
   "source": [
    "### 4c. Identify target feature -  `% wins`\n",
    "This is just for convenience as I move through the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = '% wins'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d66339",
   "metadata": {},
   "source": [
    "### 4d. Exploratory data analysis of raw data\n",
    "To get a general feel for what the raw data entails. And show why linear regression is an appropiate model for this data. WORDS ABOUT LINEAR REGRESSION ASUMPTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3196ac0",
   "metadata": {},
   "source": [
    "### 4d-1. Shape tells us how many data points and features we have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d811cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec83605",
   "metadata": {},
   "source": [
    "### 4d-2. Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d18ce3",
   "metadata": {},
   "source": [
    "### 4d-3. An overview of each columns Non-Null Count and Dtypes\n",
    "In this case, as seen above from `df.shape` and `df.isnull().sum().sum()`, there should be should be 150 non-null in all 25 columns. This also gives the data types of each columns. I *should* have all numeric features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaac8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5453a1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>4d - Notes:</b> There are no missing values, however, the above shows all the independent variables are strings; they need to be converted to numeric values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addbafbf",
   "metadata": {},
   "source": [
    "## 4e. Convert Dtypes to floats\n",
    "I can do a blanket conversion of the entire df since all features, independent and dependent, need to be float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55229a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(float)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb67ab",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Success:</b> All Dtypes are now floats.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298373b",
   "metadata": {},
   "source": [
    "## 4f. Check variability between independent features\n",
    "An important consideration when using multiple predictors in any model is the scale of these features. I will use pandas .describe() method to view details of df. \n",
    "\n",
    "Now that all Dtypes are numeric I can use pandas .describe() method to view:\n",
    "- The number of not-empty values\n",
    "- The average (mean) value\n",
    "- The standard deviation\n",
    "- the minimum value\n",
    "- The 25% percentile\n",
    "- The 50% percentile\n",
    "- The 75% percentile\n",
    "- the maximum value\n",
    "\n",
    "I will transpose it for easier viewing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98caff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e6b350",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>4f - Notes:</b> A quick scroll down the mean, min, & max columns I can see there is a huge range in each of the independent features. Variables of vastly different scales can impact the influence over the model. To avoid this, it is best practice to normalize the scale of all features before feeding the data into a machine learning algorithm. I will need to standardize my data so the features with larger numeric values are not unfairly weighted by the model. To avoid any potential data leakage, I will first split the data before altering it in anyway. </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bbbf5a",
   "metadata": {},
   "source": [
    "## 4g. Distribution of values\n",
    "When deciding which method to use when scaling, it can be helpful to understand the distribution of values so I want to do a quick histogram plot of each feature. I will use seaborn.histplot for this, documentation [HERE](https://seaborn.pydata.org/generated/seaborn.histplot.html). Presumably, the split data will have similar distributions. If I wanted to be extremely careful I could view distributions AFTER I split but in this particular case I think it is fine to view now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6469135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f5c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_grid(df=df, \n",
    "          ncols=5, \n",
    "          nrows=5, \n",
    "          figsize=(10,10), \n",
    "          title=\"Raw Data - Distribution of Values\", \n",
    "          title_fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c68b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################DELETE IF FUNCTIONS WORK\n",
    "# setup\n",
    "sns.set_style('darkgrid', \n",
    "              {'axes.facecolor': '0.9', \"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "fig, axes = plt.subplots(ncols=5, nrows=5, figsize=(10, 10))\n",
    "fig.set_tight_layout(True)\n",
    "fig.suptitle(\"Raw Data - Distribution of Values\", fontsize=20)\n",
    "\n",
    "# plot\n",
    "for index, col in enumerate(df.columns):\n",
    "    ax = axes[index//5][index%5]\n",
    "    sns.histplot(data=df[col], ax=ax, linewidth=0.1, alpha=1)\n",
    "    ax.tick_params(axis='x', rotation=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b6621d",
   "metadata": {},
   "source": [
    "# 5. Train Test Split\n",
    "Use `sklearn.model_selection.train_test_split` (documentation [HERE](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)) to create a train and test set. I will withhold 20% of data from the models learning-data, then use that 20% to test and evaluate my models performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca499c",
   "metadata": {},
   "source": [
    "## 5a. Separate data into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b55fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(target, axis = 1)\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8495352",
   "metadata": {},
   "source": [
    "## 5b. Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d1e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=.2, \n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train has a shape of: {X_train.shape}')\n",
    "print(f'X_test has a shape of: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d93e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b3b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c879d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline(steps=[('scaler', StandardScaler()),\n",
    "              ('classifier', LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e876a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae3d0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should this all be under main header PREPROCESSING?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4af544",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "# <center> Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b651d8",
   "metadata": {},
   "source": [
    "# 6. Scale Data\n",
    "\n",
    "Feature scaling is a method used to normalize the range of the independent variables of data. A machine learning algorithm can only see numbers; this means, if there is a vast difference in the feature ranges (as there is with this data, demontrated in step 4f) it makes the underlying assumption that higher ranging numbers have superiority of some sort and these more significant numbers start playing a more decisive role while training the model. Therefore, feature scaling is needed to bring every feature on the same footing. \n",
    "\n",
    "**Standardization**\n",
    "\n",
    "Feature standardization makes the values of each feature in the data have zero mean and unit variance. The general method of calculation is to determine the distribution mean and standard deviation for each feature and calculate the new data point by the following formula:\n",
    "\n",
    "$$x' = \\dfrac{x - \\bar x}{\\sigma}$$\n",
    "\n",
    "x' will have mean $\\mu = 0$ and $\\sigma = 1$\n",
    "\n",
    "Note that standardization does not make data $more$ normal, it will just changes the mean and the standard error!\n",
    "\n",
    "**Normalization**\n",
    "- Min-max scaling\n",
    " - This way of scaling brings all values between 0 and 1. \n",
    " \n",
    "$$x' = \\dfrac{x - \\min(x)}{\\max(x)-\\min(x)}$$\n",
    "\n",
    "\n",
    "- Mean normalization\n",
    " - The distribution will have values between -1 and 1, and a mean of 0.\n",
    " \n",
    "$$x' = \\dfrac{x - \\text{mean}(x)}{\\max(x)-\\min(x)}$$\n",
    "\n",
    "- You can bound your normalization range by any interval `[a,b]` with\n",
    "\n",
    "$$x' = a + \\dfrac{(x - \\min(x))(b - a)}{\\max(x)-\\min(x)}$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Choosing which method to use depends on the distribution of your data.  A couple of relevant generalizations are: \n",
    "\n",
    "- Standardization may be used when data represent Gaussian Distribution, while Normalization is great with Non-Gaussian Distribution\n",
    "- Impact of Outliers is very high in Normalization\n",
    "\n",
    "___\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Because my data is normally distributed I will use `sklearn.preprocessing.StandardScaler` default standardization (documentation [HERE](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)):\n",
    "\n",
    "$$z = (x - u) / s$$\n",
    "\n",
    "Where u is the mean of the training samples, and s is the standard deviation of the training samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528e3d3a",
   "metadata": {},
   "source": [
    "## 6a. Scale and create new scaled dfs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978bd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# fit StandardScaler on the training data only\n",
    "# convert to pandas dataframe. This is optional and a personal preference\n",
    "train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X.columns)\n",
    "\n",
    "# once fit on training data, transform testing data aswell\n",
    "test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X.columns)\n",
    "\n",
    "# view scaled training data\n",
    "train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf298a",
   "metadata": {},
   "source": [
    "## 6b. View train data with pandas .describe() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d302087",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741adcca",
   "metadata": {},
   "source": [
    "## 6c. Visualize the scaling effect\n",
    "### 6c-1. Boxplots\n",
    "seaborn.boxplot show the minimum, first quartile, median, third quartile, and maximum of features, documentation [HERE](https://seaborn.pydata.org/generated/seaborn.boxplot.html).\n",
    "Boxplots are a great way to see the change in the range of each independent feature before and after standard scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21489a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxplots(df, title=None, title_fontsize=None):\n",
    "    # set up\n",
    "   # fig, ax = plt.subplots(figsize=(10,3))\n",
    "    sns.set_style(\"darkgrid\")\n",
    "    ax = sns.boxplot(df, saturation=0.9, color=\"tab:blue\")\n",
    "    ax.tick_params(axis='x', rotation=60)\n",
    "    ax.set_xticks(range(len(df.columns)))\n",
    "    ax.set_xticklabels(df.columns, rotation=60, ha='right', rotation_mode='anchor')\n",
    "    ax.xaxis.grid(True) # Show the vertical gridlines\n",
    "    if title!=None:\n",
    "        if title_fontsize!=None:\n",
    "            ax.set_title(title, size=15)\n",
    "        else:\n",
    "            ax.set_title(title)\n",
    "\n",
    "    # plot\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931da3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots(X_train, \"BEFORE Scaling\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d210a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots(train_scaled, \"AFTER Scaling\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ffa429",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################DELETE IF FUNCTIONS WORK\n",
    "\n",
    "\n",
    "# set style\n",
    "#sns.set_style('darkgrid', \n",
    "#              {'axes.facecolor': '0.9', \"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "# X_train\n",
    "fig, ax = plt.subplots(figsize=(11,3))\n",
    "plt.boxplot(X_train, patch_artist=True, labels=X_train.columns)\n",
    "ax.tick_params(axis='x', rotation=60)\n",
    "ax.set_title(\"BEFORE standard scaling\", size=13);\n",
    "\n",
    "# train_scaled\n",
    "fig, ax = plt.subplots(figsize=(11,3))\n",
    "plt.boxplot(train_scaled, patch_artist=True, labels=train_scaled.columns)\n",
    "ax.tick_params(axis='x', rotation=60)\n",
    "ax.set_title(\"AFTER standard scaling\", size=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6487d5",
   "metadata": {},
   "source": [
    "### 6c-2. Histograms\n",
    "An additional way to view the scaling effect is through histograms. If you think of boxplots as a top view of distributions, then you can think of histograms as a side view. Imagine yourself standing on the righthand side of the above boxplots looking down the 0 line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca2e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_overlay(df, title, title_fontsize, font_scale=.5, legend_fontsize=7):\n",
    "    # set up\n",
    "    # this makes legend color boxes smaller\n",
    "    sns.set(font_scale=font_scale)\n",
    "    \n",
    "    # plot\n",
    "    fig1 = sns.histplot(data=df, palette='viridis')\n",
    "    \n",
    "    # for legend text\n",
    "    plt.setp(fig1.get_legend().get_texts(), fontsize=legend_fontsize)\n",
    "    if title != None:\n",
    "        if title_fontsize != None:\n",
    "            fig1.set_title(title, size=title_fontsize)\n",
    "        else:\n",
    "            fig1.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac68ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_overlay(X_train, \"BEFORE Scaling\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4569e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_overlay(train_scaled, \"AFTER Scaling\", 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017db50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the scale, this makes the color boxes smaller\n",
    "sns.set(font_scale=.5) # WARNING: this sets for all sns plots, reset after plotting\n",
    "\n",
    "fig1 = sns.histplot(data=X_train, palette='viridis')\n",
    "fig1.set_title(\"BEFORE standard scaling\", size=13)\n",
    "# for legend text\n",
    "plt.setp(fig1.get_legend().get_texts(), fontsize=7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e30440",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = sns.histplot(data=train_scaled, palette='viridis')\n",
    "fig2.set_title(\"AFTER standard scaling\", size=13)\n",
    "plt.setp(fig2.get_legend().get_texts(), fontsize=7);\n",
    "\n",
    "sns.set(font_scale=1) # reset scale "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ad844",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>6c - Notes:</b> You can see the means of all the independent variables are roughly around 0 and they all have roughly the same min and max values. From the boxplots, you can also see there are a few outliers, primarily in the team averaged variables.</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0654692",
   "metadata": {},
   "source": [
    "# 7. Feature Reduction\n",
    "**Regplots**\n",
    "\n",
    "Before moving on I want to reglplot all the independent variables. The first 12 features are cumulative totals of team stats and the second 12 are those same stats but divided by the number of players to get the mean. If I use both, the cumulative totals and the averages, there will be a lot of multicollinearity. Multicollinearity occurs when two or more independent variables are highly correlated with one another, and as you can imagine using one statistic to aquire the other would have a high correlation. So I need to make a choice on which set to use. I will use regplots to show me which features have a 'cleaner' linear relationship with my target variable. This method is used to plot data and a linear regression model fit. There are a number of mutually exclusive options for estimating the regression model see documentation [HERE](https://seaborn.pydata.org/generated/seaborn.regplot.html). \n",
    "\n",
    "**Heatmaps**\n",
    "\n",
    "The regplots show the relationship between the features and target variable. The heatmap will show the  correlations between all of the numeric values (in this case, all the features) in our data. The x and y axis labels indicate the pair of values that are being compared, and the color and the number are both representing the correlation. Color is used here to make it easier to find the largest/smallest numbers. The bottom row is particulary important because it shows all the features correlation with the target variable but I am also looking for strong correlation between the independent variables. Documentation [HERE](https://seaborn.pydata.org/generated/seaborn.heatmap.html).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e92abe",
   "metadata": {},
   "source": [
    "## 7a. Regplots of independent variables vs. target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c86cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regplot_grid(df, ncols, nrows, figsize, y=y_train):\n",
    "    sns.set(font_scale=.70)\n",
    "    fig, axes = plt.subplots(ncols=ncols, nrows=nrows, figsize=figsize)\n",
    "    fig.set_tight_layout(True)\n",
    "\n",
    "    for index, col in enumerate(train_scaled.columns):\n",
    "\n",
    "        ax = axes[index//ncols][index%ncols]\n",
    "        \n",
    "        sns.regplot(x = col, \n",
    "                        y = y_train, \n",
    "                        data = df, \n",
    "                        ax=ax, \n",
    "                        line_kws={\"color\": \"tab:red\"}, # change color of line\n",
    "                        scatter_kws={'s':20}, # change size of dots\n",
    "                        seed=seed)\n",
    "        ax.set_xlim(-3,3.5) # so all plots have same limits\n",
    "        ax.set_ylim(30,70) # so all plots have same limits\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a47b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "regplot_grid(train_scaled, 4, 6, (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d3623",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####################################DELETE IF FUNCTIONS WORK\n",
    "fig, axes = plt.subplots(ncols=4, nrows=6, figsize=(15, 15))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "for index, col in enumerate(train_scaled.columns):\n",
    "    ax = axes[index//4][index%4]\n",
    "    sns.regplot(x = col, \n",
    "                y = y_train, \n",
    "                data = train_scaled, \n",
    "                ax=ax, \n",
    "                line_kws={\"color\": \"tab:red\"})\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962335c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>7a - Notes:</b> It appears to me that the cumulative sum totals have either the same or even cleaner linear realtionships with the target variable. You can see this with just the scatter plots alone but the red shaded band is perhaps a better representation. This red line shows what a linear model might look like, with the shaded part being the confidence interval. The more \"noise\" the wider that red shaded area needs to be in order to explain the correlation between features. A perfectly confident model would be a clean red line without a shaded banded. I will use heatmaps to investigate further.</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d998b",
   "metadata": {},
   "source": [
    "## 7b. Heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcfba5d",
   "metadata": {},
   "source": [
    "### 7b-1. Set up for heatmap plots\n",
    "\n",
    "\n",
    "Convert y's to dfs\n",
    "\n",
    "Converting `train_scaled` and `test_scaled` to a pandas df reset the indices. I now need to convert `y_train` and `y_test` to a pandas df and reset the indices so I can concat train_scaled and y_train to  create a visualization df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test\n",
    "y_test = pd.DataFrame(y_test).reset_index().drop('index', axis=1)\n",
    "\n",
    "# y_train\n",
    "y_train = pd.DataFrame(y_train).reset_index().drop('index', axis=1)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70606d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUMULATIVE FEATURES\n",
    "\n",
    "# extract just the cumulative sum features from train_scaled\n",
    "sum_feats = train_scaled[train_scaled.columns[:12]]\n",
    "\n",
    "# create pandas dataframe with cumulative sum features and target variable\n",
    "sumz = pd.concat([sum_feats, y_train], axis=1)\n",
    "\n",
    "\n",
    "# AVERAGED FEATURES\n",
    "\n",
    "# extract just the cumulative sum features from train_scaled\n",
    "mean_feats = train_scaled[train_scaled.columns[12:]]\n",
    "\n",
    "# create pandas dataframe with cumulative sum features and target variable\n",
    "meanz = pd.concat([mean_feats, y_train], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9937b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e05dafc",
   "metadata": {},
   "source": [
    "### 7b-2. Heatmap - cumulative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf75c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(df, title=None, title_fontsize=13, font_scale=.68):\n",
    "    # plot set up\n",
    "    # change the scale, this makes text inside boxes smaller\n",
    "    sns.set(font_scale=.68) # WARNING: this sets for all sns plots, reset after plotting\n",
    "    sns.set_style('whitegrid') # Set background to white\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "    # plot heatmap\n",
    "    sns.heatmap(\n",
    "        data=df.corr(), # data to be plotted\n",
    "\n",
    "        # The mask means we only show half the values instead of \n",
    "        # showing duplicates. It's optional.\n",
    "        mask=np.triu(np.ones_like(df.corr(), dtype=bool)),\n",
    "        ax=ax,\n",
    "        annot=True, # Labels, not just colors\n",
    "\n",
    "        # Customizes colorbar appearance\n",
    "        cbar_kws={\"label\": \"Correlation\", \n",
    "                  \"orientation\": \"vertical\", \n",
    "                  \"pad\": -.01,\n",
    "                  \"extend\": \"both\", \n",
    "                  \"location\": \"right\"},\n",
    "        linewidths=.1 # white lines between boxes\n",
    "    )\n",
    "    \n",
    "    if title != None:\n",
    "        ax.set_title(title, size=title_fontsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(sumz, \"Heatmap of Correlation Between Cumulative Features (target included)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f508ce3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heatmap(meanz, \"Heatmap of Correlation Between Averaged Features (target included)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "####################################DELETE IF FUNCTIONS WORK\n",
    "\n",
    "\n",
    "# plot set up\n",
    "# change the scale, this makes text inside boxes smaller\n",
    "sns.set(font_scale=.68) # WARNING: this sets for all sns plots, reset after plotting\n",
    "sns.set_style('whitegrid') # Set background to white\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "# plot heatmap\n",
    "sns.heatmap(\n",
    "    data=sumz.corr(), # data to be plotted\n",
    "    \n",
    "    # The mask means we only show half the values instead of \n",
    "    # showing duplicates. It's optional.\n",
    "    mask=np.triu(np.ones_like(sumz.corr(), dtype=bool)),\n",
    "    ax=ax,\n",
    "    annot=True, # Labels, not just colors\n",
    "    \n",
    "    # Customizes colorbar appearance\n",
    "    cbar_kws={\"label\": \"Correlation\", \n",
    "              \"orientation\": \"vertical\", \n",
    "              \"pad\": -.01,\n",
    "              \"extend\": \"both\", \n",
    "              \"location\": \"right\"},\n",
    "    linewidths=.1 # white lines between boxes\n",
    ")\n",
    "\n",
    "ax.set_title(\"Heatmap of Correlation Between Cumulative Features (target included)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5014fe7",
   "metadata": {},
   "source": [
    "### 7b-2. Heatmap - averaged features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5ad39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################DELETE IF FUNCTIONS WORK\n",
    "\n",
    "\n",
    "# plot set up\n",
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "sns.heatmap(\n",
    "    data=meanz.corr(),\n",
    "    mask=np.triu(np.ones_like(meanz.corr(), dtype=bool)),\n",
    "    ax=ax,\n",
    "    annot=True,\n",
    "    cbar_kws={\"label\": \"Correlation\", \n",
    "              \"orientation\": \"vertical\", \n",
    "              \"pad\": -.01,\n",
    "              \"extend\": \"both\", \n",
    "              \"location\": \"right\"},\n",
    "    linewidths=.1\n",
    ")\n",
    "\n",
    "ax.set_title(\"Heatmap of Correlation Between Averaged Features (target included)\");\n",
    "\n",
    "sns.set(font_scale=1) # reset scale "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b42a647",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>7b - Notes:</b> Interestingly, the averaged variables have a few features with a stronger correlation with the target variable than those of the corresponding cumulative variables. Overall however, the cumulative features seem to have stronger correlations. In addition, there are a lot more .9 (or above) inter-feature correlations amoung the averaged variables. The averaged variables have 18 inter correlated pairs of .9 or greater, while the cumulative variables have only 5.</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6f965e",
   "metadata": {},
   "source": [
    "## 7c. Selecting Features to Drop\n",
    "Based on the Heatmap and Regplots I will *keep* the cumulative sum features. I need to eliminate the highly correlated features while keeping as much data as possible. This means I need to remove the least amount of features as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919d852a",
   "metadata": {},
   "source": [
    "![feature elimination tables](../images/elimination_tables.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b35dc",
   "metadata": {},
   "source": [
    "By eliminating `Hits Sum` & `Runs Batted In Sum` I can remove all the pairs with a correlation of .9 or greater. This leaves:\n",
    "- `Games Played Sum`\n",
    "- `At Bats Sum`\n",
    "- `Runs Sum`\n",
    "- `Doubles Sum`\n",
    "- `Triples Sum`\n",
    "- `Home Runs Sum`\n",
    "- `Walks Sum`\n",
    "- `Strikeouts Sum`\n",
    "- `Stolen Bases Sum`\n",
    "- `Caught Stealing Sum`\n",
    "\n",
    "\n",
    "**Note: this is likely not a comprehensive selection of features to be eliminated.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b754d7e3",
   "metadata": {},
   "source": [
    "## 7d. Drop Features -  Create reduced dfs from `test_scaled` & `train_scaled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ba759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train_s_r = train scaled and reduced\n",
    "train_s_r = train_scaled[train_scaled.columns[:12]].drop(['Hits Sum', \n",
    "                                                          'Runs Batted In Sum'], \n",
    "                                                         axis=1)\n",
    "\n",
    "test_s_r = test_scaled[train_scaled.columns[:12]].drop(['Hits Sum', \n",
    "                                                        'Runs Batted In Sum'],\n",
    "                                                       axis=1)\n",
    "# View train\n",
    "train_s_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e523e7d",
   "metadata": {},
   "source": [
    "# 8. Investigate Outliers\n",
    "Removing outliers can reduce the errors (or residuals) of a model. However, not all outliers should be removed, and Jim, from *Statistics By Jim*, perfectly states in [Guidelines for Removing and Handling Outliers in Data](https://statisticsbyjim.com/basics/remove-outliers/), \"Its bad practice to remove data points simply to produce a better fitting model or statistically significant results.\" \n",
    "There are other ways to deal with outliers than to do a blanketed removal. The most common is logging. Before deciding to add more complexity to my model by logging features, I need to investigate where these outliers are, and how many? Is the complexity worth it for a few data points?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb743ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplots(train_s_r, \"Show me the OUTLIERS\", 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae2792",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################DELETE IF FUNCTIONS WORK\n",
    "\n",
    "\n",
    "# set style\n",
    "sns.set_style('darkgrid', \n",
    "              {'axes.facecolor': '0.9', \"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "# X_train\n",
    "fig, ax = plt.subplots(figsize=(11,3))\n",
    "plt.boxplot(train_s_r, patch_artist=True, labels=train_s_r.columns)\n",
    "ax.tick_params(axis='x', rotation=60)\n",
    "ax.set_title(\"Show me the OUTLIERS\", size=13);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96368d9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>8a - Notes:</b> Only 4 features have outliers and it looks like they each have only 1. </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da334b0",
   "metadata": {},
   "source": [
    "I want to know exactly how many in each independent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7676ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers from a feature\n",
    "def remove_outliers(df, column):\n",
    "    \"\"\"\n",
    "    Remove outliers from a column based on zscore. If 3 \n",
    "    standard deviations away from mean the row that outlier \n",
    "    is removed from df, index is preserved, and trimmed df is\n",
    "    returned\n",
    "    \"\"\"\n",
    "    return df[(np.abs(stats.zscore(df[column])) < 3)] #outside of 3 standard deviations\n",
    "\n",
    "def print_outliers(df):\n",
    "    \"\"\"\n",
    "    Uses remove_outliers function to print out a count of outliers and the column\n",
    "    they are in\n",
    "    \"\"\"\n",
    "    # get length of entire feature\n",
    "    all_data = len(df)\n",
    "\n",
    "    # print features with outliers, and how many they have\n",
    "    for col in df.columns:\n",
    "        no_outs = len(remove_outliers(df, col))\n",
    "\n",
    "        if all_data-no_outs > 0:\n",
    "            print(f'{col} has {all_data-no_outs} outlier(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_outliers(train_s_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319d9411",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>8a - Notes:</b> These outliers are likely due to natural variation but my sample size is ralativly low. Yes it covers 5 year of data but each season only has 24 teams ie data points. I think If I were able to increase my sample size, these outliers would fall within the typical gaussian bell curve. </div> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394880e9",
   "metadata": {},
   "source": [
    "# <center> LINEAR REGRESSION MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e18b4",
   "metadata": {},
   "source": [
    "# 9. Final Visualizations of Modeling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf98f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of raw and scaled\n",
    "# I am letf with 10 stats to model with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d96841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fea23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f0016",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fff0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7539d79a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c531a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076f277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbb927b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab241114",
   "metadata": {},
   "source": [
    "## 7c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8270e0b",
   "metadata": {},
   "source": [
    "# view data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f987a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing features how to interpret, does regplot help, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e06f7c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# raw data = X_train\n",
    "# scaled = train_scaled\n",
    "# log transformed\n",
    "cont_log_df = np.log(X_train)\n",
    "\n",
    "#outliers removed from raw\n",
    "filtered_raw = X_train[(np.abs(stats.zscore(X_train)) < 3).all(axis=1)]\n",
    "raw_percent_lost = round(len(X_train)-len(filtered_raw)/len(X_train)*100,2)\n",
    "\n",
    "#outliers removed from log\n",
    "filtered_log = cont_log_df[(np.abs(stats.zscore(cont_log_df)) < 3).all(axis=1)]\n",
    "log_percent_lost = round(len(X_train)-len(filtered_log)/len(X_train)*100,2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "log_stand = pd.DataFrame(scaler.fit_transform(cont_log_df), columns=X.columns)\n",
    "\n",
    "\n",
    "cols = list(X_train.columns)\n",
    "\n",
    "for i in range(len(cols)):\n",
    "    fig, axes = plt.subplots(1, 6, figsize=(15, 3))\n",
    "    fig.subplots_adjust(top=0.8)\n",
    "    if i == 0:\n",
    "        fig.suptitle('Distribution of Values with Various Transformations', fontsize=20)\n",
    "    \n",
    "    sns.histplot(ax=axes[0], data=X_train[cols[i]], bins='auto', color='red', alpha=.5)\\\n",
    "    .set(title=\"RAW\")\n",
    "    \n",
    "    sns.histplot(ax=axes[1], data=filtered_raw[cols[i]], bins='auto', color='purple', alpha=.5)\\\n",
    "    .set(title=\"FILTERED RAW\")\n",
    "    \n",
    "    sns.histplot(ax=axes[2], data=train_scaled[cols[i]], bins='auto', color='blue', alpha=.5)\\\n",
    "    .set(title=\"RAW STANDARDIZED\")\n",
    "    \n",
    "    sns.histplot(ax=axes[3], data=cont_log_df[cols[i]], bins='auto', color='green', alpha=.5)\\\n",
    "    .set(title=\"LOG TRANSFORMED\")\n",
    "    \n",
    "    sns.histplot(ax=axes[4], data=filtered_log[cols[i]], bins='auto', color='yellow', alpha=.5)\\\n",
    "    .set(title=f\"FILTERED LOG\")\n",
    "    \n",
    "    sns.histplot(ax=axes[5], data=log_stand[cols[i]], bins='auto', color='orange', alpha=.5)\\\n",
    "    .set(title=\"LOG STANDARDIZED\")\n",
    "    \n",
    "    plt.show();\n",
    "    #i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32aa517d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\", style=\"font-size:18px\">\n",
    "<b>4f 2 - Notes:</b>  This boxplot visually expresses the min, max, mean, and percentiles of each feature. Notice the scales are so vaslty different that the roughly 2/3 of the features are uninterpretable.</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7454b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5bea5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ca0bb1d",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d488e986",
   "metadata": {},
   "source": [
    "## 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981cb291",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a9295c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fit statsmodel\n",
    "def model(df, target_variable=target):\n",
    "    y = df[target_variable]\n",
    "    X = df.drop(target_variable, axis=1)\n",
    "\n",
    "    model = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade0ff9",
   "metadata": {},
   "source": [
    "## 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d19ae71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return r_squared values, coeff /p table from .summary, and a \n",
    "# couple of residual normality checks (hist and qq plot)\n",
    "\n",
    "def model_it_small(df, target_variable=target):\n",
    "    \n",
    "    y = df[target_variable]\n",
    "    X = df.drop(target_variable, axis=1)\n",
    "    #statsmodel fit\n",
    "    model = sm.OLS(y, sm.add_constant(X)).fit()  \n",
    "    \n",
    "    #kfold\n",
    "    regression = LinearRegression()\n",
    "    crossvalidation = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    kfold_r = np.mean(cross_val_score(regression, X, y, scoring='r2', cv=crossvalidation))\n",
    "    \n",
    "    #PLOTS\n",
    "    sns.set_style('darkgrid', {'axes.facecolor': '0.9', \"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(5, 2))\n",
    "    fig.suptitle('Normality of Residuals', fontsize=8)\n",
    "    #hist\n",
    "    sns.histplot(model.resid, ax=ax0)\n",
    "    ax0.set(xlabel='Residual', ylabel='Frequency', title='Distribution of Residuals')\n",
    "    ax0.title.set_fontsize(8)\n",
    "    ax0.xaxis.label.set_fontsize(8)\n",
    "    ax0.yaxis.label.set_fontsize(8)\n",
    "    #qq\n",
    "    sm.qqplot(model.resid, fit = True, line = '45', ax=ax1)\n",
    "    ax1.set(title='QQ Plot')\n",
    "    ax1.title.set_fontsize(8)\n",
    "    ax1.xaxis.label.set_fontsize(8)\n",
    "    ax1.yaxis.label.set_fontsize(8)\n",
    "    plt.show();\n",
    "    \n",
    "    #print r_squared values\n",
    "    print(f'r_sq: {model.rsquared}. r_sq_adjusted: {model.rsquared_adj}. k_fold_r: {kfold_r}')\n",
    "    \n",
    "    #return \n",
    "    return model.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26040149",
   "metadata": {},
   "source": [
    "## 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08498001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collinearity check function\n",
    "# code from Multicollinearity of Features - Lab, turned it into a function\n",
    "\n",
    "def collinearity(df):\n",
    "    #get absolute value of correlations, sort them, and turn into new DF called df\n",
    "    df=df.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n",
    "\n",
    "    # zip the columns (Which were only named level_0 and level_1 by default) \n",
    "    # into a new column named \"pairs\"\n",
    "    df['pairs'] = list(zip(df.level_0, df.level_1))\n",
    "\n",
    "    # set index to pairs\n",
    "    df.set_index(['pairs'], inplace = True)\n",
    "\n",
    "    # drop level_ columns\n",
    "    df.drop(columns=['level_1', 'level_0'], inplace = True)\n",
    "\n",
    "    # rename correlation column as cc rather than 0\n",
    "    df.columns = ['cc']\n",
    "\n",
    "    # just correlations over .75, but less than 1.\n",
    "    df = df[(df.cc>.75) & (df.cc <1)]\n",
    "\n",
    "    df.drop_duplicates(inplace=True) \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da2ed86",
   "metadata": {},
   "source": [
    "## 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac331a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colinearity with VIF\n",
    "# code from Linear Regression - Cumulative Lab, altered to make a df w/sorted values\n",
    "\n",
    "def get_VIFs_above5(df, target_variable=target):\n",
    "\n",
    "    vif_data = sm.add_constant(df.drop(target_variable, axis=1))\n",
    "\n",
    "    vif = [variance_inflation_factor(vif_data.dropna().values, i)\\\n",
    "           for i in range(vif_data.dropna().shape[1])]\n",
    "\n",
    "    vif_df = pd.DataFrame(vif, index=vif_data.columns).sort_values(0, ascending=False)\n",
    "    return vif_df[vif_df[0]>5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1b5f6",
   "metadata": {},
   "source": [
    "## 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers from a feature\n",
    "def remove_outliers(df, column):\n",
    "    return df[(np.abs(stats.zscore(df[column])) < 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aad1e4",
   "metadata": {},
   "source": [
    "## 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a6e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeffs_as_percent_df(df, target_variable=target):\n",
    "    \"\"\"\n",
    "    takes in a pandas dataframe and target_variable \n",
    "    (intercept) as a string, ie column name, and returns\n",
    "    df with predictor coefficients as percentage_of_change \n",
    "    in target variable\n",
    "    \"\"\"\n",
    "\n",
    "    # remove target from df\n",
    "    predictors = df.drop(target_variable, axis=1)\n",
    "    \n",
    "    # import the magic sauce\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(predictors, df[target_variable])\n",
    "    \n",
    "    # get the what will become the values\n",
    "    predictors_coeffs = list(linreg.coef_)\n",
    "\n",
    "    # give it a new name\n",
    "    #all_values = [f'{round((coeff*100), 2)}%' for coeff in predictors_coeffs] # not percent, I didn't log target\n",
    "\n",
    "    # what will become the keys...\n",
    "    predictor_names = list(predictors.columns)\n",
    "\n",
    "    # give it a new name\n",
    "    all_keys = predictor_names\n",
    "\n",
    "    # zip into dictionary\n",
    "    coeff_dict = dict(zip(all_keys, predictors_coeffs))#all_values))\n",
    "    \n",
    "    # make df\n",
    "    coeff_df = pd.DataFrame(list(coeff_dict.items()))\n",
    "    \n",
    "    # rename columns\n",
    "    coeff_df.rename(columns={0: \"coeff\", 1: \"change_in_target\"}, inplace=True)\n",
    "    \n",
    "    return coeff_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c960cb6",
   "metadata": {},
   "source": [
    "## 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87207942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normality check function, returns hist of residuals and qq plot\n",
    "def normality_check(df, target_variable=target):\n",
    "    \n",
    "    # fit statsmodel with function\n",
    "    fit_model = model(df, target_variable)\n",
    "    \n",
    "    # establish plots\n",
    "    sns.set_style('darkgrid', {'axes.facecolor': '0.9', \"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 4.5))\n",
    "    \n",
    "    # histo\n",
    "    sns.histplot(fit_model.resid, ax=ax0)\n",
    "    ax0.set(xlabel='Residual', ylabel='Frequency', title='Distribution of Residuals')\n",
    "    \n",
    "    # qq plot\n",
    "    sm.qqplot(fit_model.resid, fit = True, line = '45', ax=ax1)\n",
    "    ax1.set(title='QQ Plot')\n",
    "    \n",
    "    # title for entire thing\n",
    "    fig.suptitle('Normality of Residuals')\n",
    "    \n",
    "    # show me just the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c36a9eb",
   "metadata": {},
   "source": [
    "## 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5491c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(existing_df, feature_to_add, df_adding_from=train_s_r):\n",
    "    return pd.concat([existing_df, df_adding_from[feature_to_add]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80e1601",
   "metadata": {},
   "source": [
    "## 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd2d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_plus_one(current_model_df, add_from_df=model_with):\n",
    "    print('*'*80)\n",
    "    print('CURRENT MODEL')\n",
    "    display(model_it_small(current_model_df))\n",
    "    print('*'*80)\n",
    "    print()\n",
    "    \n",
    "    model_these = add_from_df.drop(current_model_df.columns, axis=1).columns\n",
    "    \n",
    "    for feature in model_these:\n",
    "\n",
    "        features_to_add = add_from_df[feature]\n",
    "\n",
    "        expanded_model = pd.concat([current_model_df, features_to_add], axis=1)\n",
    "\n",
    "        print('-----------------------------------------------------------------------------')\n",
    "        print(f'ADDED: {feature}')\n",
    "        display(model_it_small(expanded_model))\n",
    "        print('-----------------------------------------------------------------------------')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df74aaa",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(existing_df, feature_to_add, df_adding_from=data):\n",
    "    \n",
    "    return pd.concat([existing_df, df_adding_from[feature_to_add]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "fig, axes = plt.subplots(ncols=3, nrows=3, figsize=(15, 15))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "# plot\n",
    "for index, col in enumerate(model_with.columns):\n",
    "    ax = axes[index//3][index%3]\n",
    "    sns.histplot(data=data[col], ax=ax, linewidth=0.1, alpha=1)\n",
    "    ax.tick_params(axis='x', rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1a6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with.describe().T.sort_values('std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get absolute value of correllations and sort highest to lowest\n",
    "model_with.corr()[target].abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635faac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "# Set up figure and axes\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot a heatmap of the correlation matrix\n",
    "sns.heatmap(\n",
    "    # Specifies the data to be plotted\n",
    "    data=model_with.corr(),\n",
    "    # The mask means we only show half the values instead of showing duplicates. It's optional.\n",
    "    mask=np.triu(np.ones_like(model_with.corr(), dtype=bool)),\n",
    "    # Use the existing axes\n",
    "    ax=ax,\n",
    "    # Labels, not just colors\n",
    "    annot=True,\n",
    "    # Customizes colorbar appearance\n",
    "    cbar_kws={\"label\": \"Correlation\", \n",
    "              \"orientation\": \"vertical\", \n",
    "              \"pad\": -.01,\n",
    "              \"extend\": \"both\", \n",
    "              \"location\": \"right\"},\n",
    "    # white lines between boxes\n",
    "    linewidths=.1\n",
    ")\n",
    "\n",
    "# Customize the plot appearance\n",
    "ax.set_title(\"Heatmap of Correlation Between Attributes (Including Target)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655992cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most correlated feature is "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab9cb2",
   "metadata": {},
   "source": [
    "# Base model with most correlated feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8ba850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 'team_rbi' as baseline model feature\n",
    "# baseline model\n",
    "baseline_model_df = model_with[['team_rbi', target]]\n",
    "\n",
    "baseline = model(baseline_model_df)\n",
    "baseline.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7331c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a look at the residuals\n",
    "normality_check(baseline_model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ddedae",
   "metadata": {},
   "source": [
    "# Add each feature to base model individually and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62decf56",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# model adding one feature to baseline\n",
    "model_plus_one(baseline_model_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524665ca",
   "metadata": {},
   "source": [
    "# Add feature with lowest p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdccf0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "two_feats = combine(baseline_model_df, 'team_ab')\n",
    "model_plus_one(two_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f251ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "three_feats = combine(two_feats, 'obp_mean')\n",
    "model_plus_one(three_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90fb144",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "four_feats = combine(three_feats, 'slg_mean')\n",
    "model_plus_one(four_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbdb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(four_feats, target).summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4920314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "normality_check(four_feats, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006d262",
   "metadata": {},
   "outputs": [],
   "source": [
    "collinearity(four_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e4aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_VIFs_above5(four_feats, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = four_feats.drop(target, axis=1)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(8, 5))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "for index, col in enumerate(plots.columns):\n",
    "    ax = axes[index//2][index%2]\n",
    "    sns.regplot(x = col, y = target, data = data, ax=ax, line_kws={\"color\": \"tab:red\"})\n",
    "    ax.set_xlabel(col)\n",
    "    ax.set_ylabel(\"win rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb26d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassigning to more descriptive variable\n",
    "final_model = model(four_feats, target)\n",
    "\n",
    "# plot the residuals against predicted values\n",
    "y_pred = final_model.fittedvalues\n",
    "\n",
    "# check for homoscedasticity\n",
    "p = sns.scatterplot(x=y_pred,y=final_model.resid)\n",
    "plt.xlabel('Predicted y values')\n",
    "plt.ylabel('Residuals')\n",
    "#plt.xlim(70,100)\n",
    "p = sns.lineplot(x=[y_pred.min(),y_pred.max()],y=[0,0],color='blue')\n",
    "p = plt.title('Residuals vs Predicted y value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_as_percent_df(four_feats, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67603cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ccc3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e90f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395152c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d7fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc56cf50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xgboost\n",
    "#from xgboost import XGBClassifier\n",
    "\n",
    "#from imblearn.over_sampling import SMOTEN, SMOTENC\n",
    "#from imblearn.pipeline import Pipeline as imbpipeline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, \\\n",
    "ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler#, OneHotEncoder\n",
    "#from sklearn.compose import ColumnTransformer\n",
    "#from sklearn.experimental import enable_iterative_imputer\n",
    "#from sklearn.impute import IterativeImputer, SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix,\\\n",
    "    precision_score, recall_score, accuracy_score, f1_score, log_loss,\\\n",
    "    roc_curve, roc_auc_score, classification_report, plot_roc_curve, auc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e43e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e79942e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bf4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define GridSearchCV Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978e7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lists for the Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afb57be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Empty results Data Frame to Store Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop Through all Models, Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff09030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da0dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72f2a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa45038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d1a9bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLB-env",
   "language": "python",
   "name": "mlb-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
